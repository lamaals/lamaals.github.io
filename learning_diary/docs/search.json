[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "About\nHi, I’m Lama, I come from a background in architecture and urban planning. Following my undergraduate studies, I pursued an MSc in Construction Science and Implementation Techniques, in our dissertation, my partners and I innovated a novel clay material characterized by its sustainability and remarkable durability, capable of withstanding substantial tension and pressure forces. This achievement contributed in earning a distinction degree. During my previous master’s program, I worked as a senior architect, overseeing the design and construction of buildings across various functions. Additionally, I actively contributed to the regeneration and planning efforts in multiple areas within my country. Driven by my interest in the transformative potential of urban planning and the role of smart cities in societal advancement, I decided to pursue an MSc in Urban Spatial Science at UCL. I’m enthusiastic about exploring how data can be leveraged to create and enrich the fabric of our communities, so this is me and here I’m.\nBeside Architecture, urban planning and construction I really love taking photos , capturing beautiful moments, and travailing around the world\n\n\n\nMe on of the construction site (on the corner of a flying canopy)\n\n\n\n\n\nCaptured by me."
  },
  {
    "objectID": "Week 1.html",
    "href": "Week 1.html",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "",
    "text": "References\nEvaluation and comparison of Sentinel-2 MSI, Landsat 8 OLI, and EFFIS data for forest fires mapping. Illustrations from the summer 2017 fires in Tunisia. (Achour.H, Toujani.A 2021).\nSatellites capture socioeconomic disruptions during the 2022 full‑scale war in Ukraine. (Ialongo.I, Bun.R 2023)."
  },
  {
    "objectID": "Week 1.html#summary",
    "href": "Week 1.html#summary",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nFor the first lecture in this module, we got introduced to the world of remote sensing cities, where we discovered the methods used for data collection using sensors that are mounted on diverse platforms (space-borne/airborne). Then we discussed the two types of sensors: passive which perceive natural radiation, and active, which radiate their own radiation. Then we explored the differences in terms of spectral bands, spatial resolution, and revisit frequency between Sentinel 2 and Landsat satellites. Finally, the attractive part for me was to know that electromagnetic radiation can be obstructed by surfaces or atmosphere which cause absorption,transmission, or scattering.\n\n\n\nFigure (1): Active and Passive sensors. (“Technology  Balamis,” n.d.)\n\n\n\n1.1.1 Remote sensing definition\nAccording to NASA “Remote sensing is the acquiring of information from a distance” (Earth Science Data Systems 2019). By observing the energy response (reflected, transmitted) and recording information then use it in responding to hazards, apply energy studies utilizing, urban planning, and environmental treaty enforcement…..etc.\n\n\n1.1.2 The process of remote sensing\nElectromagnetic energy, created by moving charged particles, travels in waves through space and the atmosphere. These waves vary in length and frequency; shorter waves have higher frequencies. Electromagnetic waves might get obstructed by a surface or atmosphere which cause scattering for the short wave such as blues and then it reflects back to be recorded and processed to information/data.\n\n\n\nFigure (2): the remote sensing process. (“Remote Sensing from Space  Paititi Research,” n.d.)\n\n\n\n\n1.1.3 Sentinel-2 VS Landsat\nSentinel-2 contains two satellites orbiting around the earth in the same path, but 180° spaced apart. These two satellites have a rapid visit time of 10 days at the equator and a wide area of coverage as they are designed to continuously detect changes in land surface conditions. (“Sentinel-2 - Missions - Sentinel Online,” n.d.)\nLandsat 8 holds two main instruments which are: the Operational Land Images (OLI) and the Thermal Infrared Sensor (TIRS). These sensors works on providing images of the earth surface at several resolution, and that includes for visible, near-infrared, and short infrared a 30m, for Thermal data 100m, and for panchromatic data a 15m. (“Landsat 8 | Landsat Science” 2021)"
  },
  {
    "objectID": "Week 1.html#application",
    "href": "Week 1.html#application",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "1.2 Application",
    "text": "1.2 Application\nLandsat and Sentinel 2 have been used extensively in different fields including land use planning and monitoring, emergency response and management, water use monitoring and others. Each one of those two methods has its strengths and to understand those strengths, I will discuss an analysis that was conducted to evaluate and compare two data sources in forest fires mapping which is: Evaluation and comparison of Sentinel-2 MSI, Landsat 8 OLI, and EFFIS data for forest fires mapping. Illustrations from the summer 2017 fires in Tunisia : (Achour et al. 2022)\nThis analysis compares and assesses spectral indices derived from Sentinel-2 and Landsat-8 OLI imagery and to identify the most appropriate index for each sensor for accurately mapping forest fires, using as a case study two fire events that occurred in summer 2017 in northern Tunisia (Achour et al. 2022, Toujani et al. 2022).\n\n\n\nFigure (3): Sentinel 2A Multi Spectral Instrument (MSI) and Landsat-8 Operational Land Imager (OLI) satellite images were acquired before (pre-fire) and after (post-fire) the fires from the U.S. Geological Survey (USGS)\n\n\n\n1.2.1 Discussion\nResults from this study highlight those spectral indices including NIR-SWIR bands derived from Landsat 8 OLI and Sentinel-2 MSI (ΔNBRn, ΔNBR and RBR) had higher discriminatory power than classical indices based on NIR and red bands (BAI, ΔNDVI, ΔEVI, and ΔMSAVI).\nIn the figure below, we can see that spectral indices derived from Landsat 8, especially those combining the Red-NIR bands such as ΔNDVI, ΔEVI and ΔMSAVI exhibited a higher spectral separability as compared to their counterparts generated from Sentinel bands in both fires.\n\n\n\nFigure (4): M-statistic values to discern burned/unburned areas for both Sentinel 2 (S) and Landsat-8 OLI (L) spectral indices\n\n\nMaps of the estimated burned areas from each spectral index and for each fire reveals that both Landsat 8 and Sentinel-2A can capture the boundaries of fires identified using Copernicus EMS reference maps, albeit with small differences between burned areas, retrieved from each sensor.\nSentinel-based spectral indices (ΔNBRn and RBR) provided the highest level of positional accuracy, with a loc value of about 60 metres. This means that the positions of the extracted fires were shifted approximately by three pixels from their positions in the reference map. However, Landsat-based spectral indices, RBR and ΔNBR, displayed a loc value of about 75 meters, suggesting that the position of the extracted fires were shifted approximately 2.5 pixels from their positions in the reference map. The advantage of Sentinel 2A in terms of positional accuracy could be attributed to its higher spatial resolution as compared to Landsat 8.\n\n\n\nFigure (5): Distribution of burned and unburned values for spectral indices selected for F1 / F2\n\n\n\n\n\nFigure (6): Spectral indices vs. Copernicus EMS fire polygon for the Haddada fire\n\n\n\n\n1.2.2 Conclusion\nThe study concluded that (1) regardless of the sensor, spectral indices that incorporated NIR-SWIR bands exceed those using red and NIR bands in terms of spectral separability; (2) from the viewpoint of accuracy, the Sentinel sensor is slightly more efficient than the Landsat 8 in mapping burned scars, but both sensors produce similar and acceptable results. Hence, this paper concludes that both sensors are a good alternative to EFFIS data, particularly when there is a need to detect details inside the fire. From those results we can clearly see that both Sentinel and Landsat 8 sensors are very powerful in detecting burned areas affecting forests or residential areas, this can be applied in different context whether in responding to catastrophes or implementing new policies to protect those affect areas. Obviously this can be expanded in further studies and applied to other environmental issues like deforestation and many more."
  },
  {
    "objectID": "Week 1.html#reflection",
    "href": "Week 1.html#reflection",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThis lecture wasn’t easy to digest at first, as there was a lot of new ideas and terms (Spectral, Spectrum, Spectral Signature…etc ) to learn. when I went through the lecture it was captivating the facts that we learned for example the scattering of EMR and how the process of gathering satellite images is not as easy and simple as i thought, plus it was really interesting knowing why the sky and sea are blue. This lecture opened my eyes to the uses of satellite data and the importance of it and how it can be used in different areas and in very different methods but what was really interesting for me is the paper that i found on the use of satellite images in capturing the socioeconomic disruption during the Ukrainian war. In this study they combined satellite data on nitrogen dioxide (NO2) and carbon dioxide (CO2) to track changes in human activities, as both gases are linked to fossil fuel combustion. the analysis reveals a significant decrease in NO2 levels over major Ukrainian cities, power plants, and industrial areas during the second quarter of 2022, ranging from 15% to 46% compared to the reference period of 2018-2021. They also used the detection of unusual fire which might be likley from shelling rather than agricultural burning (Ialongo et al. 2023).These findings demonstrate the value of satellite observations in monitoring significant societal changes, particularly during conflicts."
  },
  {
    "objectID": "week 2.html#summary",
    "href": "week 2.html#summary",
    "title": "\n2  Week 2 Xaringan: Crafting Stunning Presentations\n",
    "section": "\n2.1 Summary",
    "text": "2.1 Summary\nThe content of week 2 was about making presentation and learning diaries. Below is presentation that was made using Xaringan about GOSAT-2 satellite."
  },
  {
    "objectID": "week 3.html#summary",
    "href": "week 3.html#summary",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nFor this week, we were introduced to Push Broom and Whisk Broom sensors, including differences between them. Then we explored Geometric Correction, Atmospheric Correction, Topographic Correction, and Radiometric which are all methods used in the correction of remote sensing products, as satellite images usually might have flaws. Furthermore, we continued to cover the techniques for joining data and applying enhancement on satellite images.\n\n3.1.1 Geometric Correction\nFirst the coordination’s of the dataset are fed into a mathematical model in order to specify the transformation coefficients then in order to execute a geometric transformation the coefficients will be used which eventually lead to effectively altering the spatial arrangement in the dataset. In other words it is the “ attempt to correct for positional errors and to transform the original image into a new image that has the geometric characteristics of a map. can you rephrase“(Fagan and DeFries 2024)\n\n\n\nFigure (1): Geometric Correction process. (“Geometric Corrections - AWF-Wiki,” n.d.)\n\n\n\n\n3.1.2 Atmospheric Correction\nAtmospheric correction is the process of scattering and absorption effect removal which are caused by the atmosphere, thus showing the surface reflectance that represents surface properties. (GISGeography 2015)\nMethods to remove the atmosphere effect:\nRelative: This method facilitates the comparison with other images instead of directly removing atmospheric effects by normalising images. Relative correction most common technique is Dark Object Subtraction (DOS), where dark objects with very low reflectance are deducted from the image. This procedure allow the standardization of the images and the mitigation of atmospheric effects.\n\n\n\nFigure (2) Dark pixel subtraction. (“Marine Remote Sensing Toolkit,” n.d.)\n\n\nAbsolute: Includes the evaluation of the atmospheric conditions along with the angles of illumination and observation during the capture of the image in order to assess the level of scattering and absorption for each image band. Using these factors, the correction factors are calculated to modify the data from its original at-sensor values to establish more accurate at-surface values. (“Atmospheric Remote Sensing Toolkit,” n.d.)\nEmpirical line method: This technique provides an replacement to radiative transfer modelling approaches. It offers a clear approach to calibrating surface reflectance, unless the accessibility of calibration measurements that are static over time is guaranteed.\nPhases of Atmospheric Correction\nAtmospheric Correction is a process that contain two phases. In the first phase, Digital Numbers (DNs) are converted into radiance, and after that into top-of-atmosphere. In the next phase, top-of-atmosphere reflectance is transformed into surface reflectance (known as bottom-of-atmosphere reflectance) this is also known as top-of-canopy reflectance, especially in vegetation studies.\n\n\n\nFigure (3) Phases of Atmospheric correction. (26atm?)"
  },
  {
    "objectID": "week 3.html#application",
    "href": "week 3.html#application",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.2 Application",
    "text": "3.2 Application\nApplying atmospheric corrections on satellite images is a crucial step that should be performed before conducting any further analysis or processing using those images. However, as we saw earlier, there are multiple approaches with different techniques that varies from image-based empirical correction to model-based methods. Each one of these techniques has its strength and weakness and, in this section, we will highlight two main studies that compared between different approaches like (QUAC, FLAASH and DOS). Those studies are “Comparison and evaluation of atmospheric correction algorithms of QUAC, DOS and FLAASH for HICO hyperspectral imagery” (Shi et al. 2016) and “A Comparison of Image-Based and Physics-Based Atmospheric Correction Methods for Extracting Snow and Vegetation Cover in Nepal Himalayas Using Landsat 8 OLI Images” (Niraj, Gupta, and Shukla 2022).\nComparison for HICO hyperspectral imagery paper:\nThis study concludes that all three types of corrections (FLAASH, QUAC, DOS) can remove the effect of atmosphere for HICO hyperspectral image. It also confirms that based on analysing the situ data, FLAASH model have a better performance than QUAC and DOS methods for reducing effect of atmosphere HICO images. Having said that, both QUAC and DOS depend on less input parameters and their computational speed is much faster than FLAASH.\n\n\n\n\n\nComparison for extracting snow and vegetation cover in Nepal Himalayas paper:\nIn this study, eight correction methods were applied on Landsat 8 OLI satellite image to find the best model for mapping snow and vegetation covered areas. The study found that FLAASH and 6SV methods determined best snow reflectance values, while DOS3 and QUAC were the worst. Additionally, FLAASH and SIAC methods showed greater vegetation reflectance values and higher ranks of correlating the extracted vegetation spectra with the standard spectra while DOS and QUAC were the lowest.\nThe study found when compared to other image-based correction methods (QUAC, Aref, COST, DOS, and DOS3), the FLAASH, SIAC, and 6SV methods generate higher snow and vegetation mean reflectance values, thus having a high possibility of mapping true snow and vegetation features.\n\n\n\n\n\n\n\n\n\n\n\n3.2.0.1 Conclusion\nBoth studies explore different atmospheric correction methods for remote sensing imagery, with a focus on varying types of imagery and target variables. The first study evaluates FLAASH, QUAC, and DOS correction methods for HICO hyperspectral imagery, concluding that FLAASH performs better in removing atmospheric effects despite its slower computational speed. In contrast, the second study assesses eight correction methods applied to Landsat 8 OLI satellite imagery to map snow and vegetation cover in the Nepal Himalayas, finding that FLAASH and SIAC methods produce superior results for snow and vegetation reflectance. Ultimately, it’s not possible to determine that one method is superior to another. Therefore, the selection of an atmospheric correction method relies on the specific objectives and characteristics of the remote sensing analysis."
  },
  {
    "objectID": "week 3.html#reflection",
    "href": "week 3.html#reflection",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nTo be honest, today’s lecture was kind of challenging and long, which makes it hard to fully understand. To work on satellite images can by tricky, yet it’s important to be able to manipulate them which can prove invaluable for emphasising specific points. It is crucial to understand the various correction methods, as each method serves a specific purpose. Furthermore, the selection of the appropriate approach depends on the variables and characteristics of the analysis. While I really hope to avoid performing correction myself, but I think It is important to know how to apply these methods and when."
  },
  {
    "objectID": "week 3.html#reference",
    "href": "week 3.html#reference",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.4 Reference",
    "text": "3.4 Reference\nComparison and evaluation of atmospheric correction algorithms of QUAC, DOS and FLAASH for HICO hyperspectral imagery (Shi, Mao, 2016).\nA Comparison of Image-Based and Physics-Based Atmospheric Correction Methods for Extracting Snow and Vegetation Cover in Nepal Himalayas Using Landsat 8 OLI Images (Niraj, Gupta 2022).\n\n\n\n\n“Atmospheric Remote Sensing Toolkit.” n.d. https://sees-rsrc.science.uq.edu.au/rstoolkit/en/html/atmospheric/resources/fundamentals/corrections/physics-based.html.\n\n\nFagan, Matthew E., and Ruth S. DeFries. 2024. “Remote Sensing and Image Processing.” In, edited by Samuel M. Scheiner, 432–45. Oxford: Academic Press. https://doi.org/10.1016/B978-0-12-822562-2.00060-8.\n\n\n“Geometric Corrections - AWF-Wiki.” n.d. http://wiki.awf.forst.uni-goettingen.de/wiki/index.php/Geometric_corrections.\n\n\nGISGeography. 2015. “What Is Atmospheric Correction in Remote Sensing?” https://gisgeography.com/atmospheric-correction/.\n\n\n“Marine Remote Sensing Toolkit.” n.d. https://sees-rsrc.science.uq.edu.au/rstoolkit/en/html/marine/resources/fundamentals/corrections/atmospheric_relative-1.html.\n\n\nNiraj, K. C., Sharad Kumar Gupta, and Dericks Praise Shukla. 2022. “A Comparison of Image-Based and Physics-Based Atmospheric Correction Methods for Extracting Snow and Vegetation Cover in Nepal Himalayas Using Landsat 8 OLI Images.” Journal of the Indian Society of Remote Sensing 50 (12): 2503–21. https://doi.org/10.1007/s12524-022-01616-6.\n\n\nShi, Liangliang, Zhihua Mao, Peng Chen, Sha’ou Han, Fang Gong, and Qiankun Zhu. 2016. “Remote Sensing of the Ocean, Sea Ice, Coastal Waters, and Large Water Regions 2016.” In, 9999:259–63. SPIE. https://doi.org/10.1117/12.2241368."
  },
  {
    "objectID": "week 4.html#summary",
    "href": "week 4.html#summary",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nIn this week, we focused on applying remote sensing to accomplish policy objectives. Then we were asked to select a city and identify policy challenges that could be then addressed using remote sensing. I’m really intrigued by the observation and analysis of cities affected by conflict. I believe that by leveraging satellite images we can assess the economic, environmental, and social impact of war, which can help in the decision-making process. Hence, I have chosen to study the impact of war on Ukraine.\nAs highlighted in the Sustainable Development Goals agenda:\nGoal 11: Make cities and human settlements inclusive, safe, resilient and sustainable.In fact, sustainable development in cities is critical to achieving most of Agenda 2030. (“Universal Sustainable Development Goals .:. Sustainable Development Knowledge Platform,” n.d.)"
  },
  {
    "objectID": "week 4.html#application",
    "href": "week 4.html#application",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.2 Application",
    "text": "4.2 Application\nBecause of the war on Ukraine the economics of the country has been massively impacted, for the first year of the conflict the GDP of Ukraine saw a devastating 30-35% decline. The deterioration of the economic in Ukraine is one of many side effects of war (other impacts such as displacement, infrastructure damage, pollution, agriculture damage, etc..).\nThe focus of the new agenda is to prioritize the development of cities that exhibit resilience, environmental sustainability, and economic growth in urban areas. Therefore, it was appropriate to highlight Ukraine, a country in conflict, as a case study.\nIt is appropriate to use remotely sensed data in this kind of sensitive situation, as it can provide accurate information about it. This was illustrated throw some of studies that was conducted on Ukraine, these studies helped in observing socioeconomic changes, air pollution, infrastructure damage, and aid in the refugee relief operation. For this week I will focus on only one paper while also recommending other remote sensing methods that can aid in studying the impact of war. This paper details the methods used for damage assessment: War Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Image (Aimaiti et al. 2022).\nIn this study, medium-resolution satellite imagery was used because it is publicly available and capable of rapidly producing damage maps, which can serve as an initial reference for assessing damage. To focus on urban areas and suppress changes from other features and landcover types, a built-up area mask is generated using OpenStreetMap building footprints and the World Settlement Footprint (WSF) dataset, respectively.\n\n4.2.0.1 Methods\nThe workflow consists of four phases: data selection and pre-processing, change detection analysis using the log ratio of intensity and Gray Level Co-occurrence Matrix (GLCM) methods, determination of an optimal threshold for separating changed and unchanged areas, and validation of results using high-resolution WorldView images and the UNOSAT damage assessment map.\n\n\n\nFigure (1): the processing workflow for building damage mapping using Sentinel-1 and Sentinel-2 images. Note: dif, AOI, and Tr refer to the difference, selected area for threshold determination, and threshold.\n\n\nSAR Intensity Analysis:\nA collapsed building exhibits a distinct backscatter structure compared to an intact one. Typically, strong backscattering diminishes or disappears after a building collapses due to a disaster. However, in partially damaged buildings, where sections of the wall and roof collapse, remaining walls, debris, and ground may form a corner reflector, resulting in strong double-bounce effects and increased backscattering intensity. This difference enables the identification of damaged and undamaged buildings using SAR backscattering intensity change detection. For this purpose, the study implemented the simple yet robust SAR log ratio of intensity method.\n\n\n\nFigure (2): schematic diagram of backscatter intensity from (a) intact buildings (b) destroyed buildings, and (c) partially damaged buildings in synthetic aperture radar (SAR) images; (a-1,a-2,b-1,b-2,c-1,c-2) are pre-and post-event Sentinel-1 images (19 February 2022 & 8 April 2022); (a-3,b-3,c-3) are corresponding WorldView images (25 & 31 March 2022).\n\n\nOptical Texture Analysis:\nIn this study, the GLCM Mean was utilized to differentiate between damaged and undamaged buildings, This feature is particularly valuable for classifying landscapes, especially in building damage assessment. GLCM Mean represents interior texture, characterized by high values in areas with subtle and irregular variations and few coherent edges. Statistics were calculated with a 3 × 3 window size using ENVI software. Initially, the average of pre-event images was computed to create a single pre-event texture image. Subsequently, the texture difference between this image and the post-event texture was calculated. The optimal threshold value for texture difference analysis was determined to be 0.4, with pixels below this threshold classified as damaged.\n\n\n\nFigure (3): the GLCM texture difference and the corresponding optical images. (a–d) are selected examples that have significant texture changes, and those locations are shown on the WorldView-2 image\n\n\n\n\n4.2.0.2 Results\nThe Sentinel-1 intensity analysis revealed flooding along the Irpin River. This area was excluded from the assessment as the flooding might be an indirect impact of war, but the focus is specifically on war-related damages to buildings and infrastructure. Significant building damage was concentrated in the northwestern part of Kyiv Oblast, particularly in Irpin, Bucha, and Hostomel. Building statistics, ranging from 85 m2 to 74,156 m2, were calculated using augmented OSM polygons and ArcGIS Desktop software. Most buildings identified by SAR intensity analysis were medium to large-scale structures. Smaller residential buildings were poorly detected, likely due to sensor limitations. High-rise buildings were flagged as damaged, but visual assessments using WorldView imagery did not confirm this finding. The WSF mask showed more false positives than the OSM mask.\n\n\n\nFigure (4): comparison of Sentinel-1 damaged building results using two different masks (OSM&WSF). (a) shows the SAR intensity-based results for the entire area. The locations (b–e) were selected as representative areas for comparison with the high-resolution WorldView image. The columns from left to right indicate the WorldView image (31 March 2022), OSM and WSF mask results of the damaged buildings. The scale shown in (b) is the same for (c–e).\n\n\nThe damage assessment results from the Sentinel-2 texture analysis revealed a flooded area along the Irpin River, similar to the SAR intensity analysis but with a smaller extent. Four representative areas were selected for comparison using the OSM mask versus the WSF built-up layer mask. The effects of these masks were less significant compared to the SAR intensity analysis, indicating fewer false positives in unmasked damage labels. Comparison with high-resolution WorldView images showed that the texture analysis mainly detected damages on large buildings with clear boundaries. However, buildings with dark roofs and smaller damaged buildings lacking distinct texture features compared to their surroundings were not captured in the analysis. Additionally, changes in intensity values of bright objects, representing either bare ground or tall structures showing different reflectance at different times, were misclassified as damaged buildings.\n\n\n\nFigure (5): the result of Sentinel-2 texture-based analysis for study region (a). (b–e) are comparison of Sentinel-2 damaged building results using two different masks (OSM&WSF). The locations (b–e) are referenced in (a) and were selected as representative areas for comparison with the high-resolution WorldView image. The columns from left to right indicate the WorldView image (25 March 2022 & 31 March 2022), OSM and WSF mask results of the damaged buildings. The scale shown in (b) is the same for (c–e).\n\n\n\n\n4.2.0.3 Conclusion\nThis study utilized Sentinel-1 and Sentinel-2 data to assess damage in Kyiv during the 2022 war. The analysis revealed concentrated damage in the northwestern Kyiv Oblast, particularly in cities like Bucha, Irpin, and Hostomel. When comparing damage detection algorithms, the analysis achieved 58% accuracy compared to UNOSAT verified damage, with 76% accuracy for larger buildings. The use of OpenStreetMap (OSM) and World Settlement Footprint (WSF) masks reduced false positives, but incomplete OSM footprints excluded some damage. Although texture analysis effectively detected large damaged buildings, it was less accurate for identifying small ones. Consequently, the study found that the intensity analysis performed better due to its ability to identify a wider range of building sizes accurately.\n\n\n4.2.0.4 Other methods that can be implemented in conflict zones using remote sensed data:\nEvaluation of Socio-Economic Consequences of war :\nThis can be done through the use of couple of methods like:\n\nusing satellite observations of nitrogen dioxide (NO2) and carbon dioxide (CO2) to monitor changes in human activities, such as fossil fuel combustion, power generation, and industrial production, during the conflict. (Ialongo et al. 2023)\nDistinct fire and thermal anomaly patterns due to military activities, contrasting with past agricultural-related fires. These patterns, can be confirmed by Sentinel 2 imagery, carry significant socio-economic implications, straining recovery efforts and requiring resource allocation based on various factors.\nSatellite data from the VIIRS thermal imager utilization to monitor socio-economic developments, using Total Light Intensity (TLI) serving as an indicator of night illumination, reflecting economic activity, population density, and electricity consumption.\n\nMonitor environment and air quality (Savenets et al. 2023)\n\nEmploying NO2 and CO satellite data, filtering the data fields using a quality assurance index and binned into regular grids for reliability and comparison.\nDetection of environmental changes: remote sensing enables the identification of alterations in natural settings, including deforestation, water pollution, shifts in vegetation cover, among others, potentially stemming from military activities.\n\nThese are several ways in which remotely sensed data can contribute to urban expansion and guide post-war urban planning initiatives to construct a more resilient city."
  },
  {
    "objectID": "week 4.html#reflection",
    "href": "week 4.html#reflection",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nConducting research on policies has expanded my understanding of how remote sensing data can drive transformation and development in urban areas, environmental monitoring, and socio-economic analysis. This comprehensive approach underscores the adaptability of remote sensing and the various methods tailored to specific case studies. Despite its potential, remote sensing also poses limitations, such as restricted access to high-resolution data and potential inaccuracies in results. Nonetheless, I firmly believe that leveraging remote sensing methodologies and analyses can inform decision-making processes crucial for national and international development."
  },
  {
    "objectID": "week 4.html#reference",
    "href": "week 4.html#reference",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.4 Reference",
    "text": "4.4 Reference\nWar Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Images (Aimaiti, Koch 2022).\nSatellites capture socioeconomic disruptions during the 2022 full‑scale war in Ukraine (Ialongo, Hakkaraine 2022).\nRemotely visible impacts on air quality after a year-round full-scale Russian invasion of Ukraine (Savenets, Osadchyi 2023).\n\n\n\n\nAimaiti, Yusupujiang, Christina Sanon, Magaly Koch, Laurie G. Baise, and Babak Moaveni. 2022. “War Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Images.” Remote Sensing 14 (24): 6239. https://doi.org/10.3390/rs14246239.\n\n\nIalongo, Iolanda, Rostyslav Bun, Janne Hakkarainen, Henrik Virta, and Tomohiro Oda. 2023. “Satellites Capture Socioeconomic Disruptions During the 2022 Full-Scale War in Ukraine.” Scientific Reports 13 (1): 14954. https://doi.org/10.1038/s41598-023-42118-w.\n\n\nSavenets, Mykhailo, Volodymyr Osadchyi, Kateryna Komisar, Natalia Zhemera, and Andrii Oreshchenko. 2023. “Remotely Visible Impacts on Air Quality After a Year-Round Full-Scale Russian Invasion of Ukraine.” Atmospheric Pollution Research 14 (11): 101912. https://doi.org/10.1016/j.apr.2023.101912.\n\n\n“Universal Sustainable Development Goals .:. Sustainable Development Knowledge Platform.” n.d. https://sustainabledevelopment.un.org/index.php?page=view&type=400&nr=1684&menu=1515."
  },
  {
    "objectID": "week 6.html#summary",
    "href": "week 6.html#summary",
    "title": "5  Week 5 GEE: Mapping the world",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIt’s week 6 now, and for this week we got introduced to Google Earth Engine(GEE) and its features, we covered the advantages of GEE usage and the several procedures it facilitates. While in the lecture it became obvious that using GEE would be more straightforward compared to QGIS and SNAP. This was such a relief and an exciting prospect, due to the reputation of GEE's ad a leading platform for analysing remote sensing data.\n\n5.1.0.1 What is GEE?\nBecause of the enormous amounts of data captured from satellites, analysing data became more challenging. GEE has appeared as the solution to process and analyse big data. Basically, GEE is a cloud-based platform that can process large geospatial datasets across wide areas and observe environmental changes over extended periods."
  },
  {
    "objectID": "week 6.html#application",
    "href": "week 6.html#application",
    "title": "5  Week 5 GEE: Mapping the world",
    "section": "5.2 Application",
    "text": "5.2 Application\nGEE is applied in various applications and is extensively employed around the world for different studies and analyses. That been said, it is crucial to briefly examine the importance of GEE, its functions, applications, benefits, and limitation in remote sensing analysis. This will be based on two papers which are: Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review (Tamiminia et al. 2020) and Google Earth Engine for geo-big data applications: A meta-analysis and systematic review. (Amani et al. 2020)\n\n5.2.0.1 Why Google earth engine.\nGEE enables users to access an archive of over a petabyte of satellite imagery and geospatial datasets, hosted on computing infrastructure provided by Google, making it an optimal setting for performing detailed analyses on satellite imagery and remote-sensing data. Integrated with a comprehensive library of historical and contemporary satellite imagery and robust APIs, GEE enables users to perform complex geospatial analyses directly using the computing power of the cloud. Unlike resorting to conventional analysis using locally downloaded satellite data, GEE mitigates the demand for large data downloads through its ability to directly process dense geospatial datasets in the cloud. In sum, GEE provides comprehensive access to geospatial data, and its cloud-based computing environment further allows it to seamlessly execute sophisticated analyses on satellite imagery and remote-sensing data.\n\n\n5.2.0.2 GEE functions\nGoogle Earth Engine offers a wide range of functions for performing spectral and spatial operations on single or multiple images. While it supports various pixel-based spectral operations that can be efficiently implemented in parallel on cloud architecture, it has limited support for spatial functions such as filters, edge detection methods, line detection via Hough Transform, and morphological operators due to parallel implementation challenges. However, it provides access to supervised and unsupervised machine learning algorithms including CART, SVM, RF classifiers, and clustering algorithms like K-means, Cascade K-means, X-means, Cobweb, and SNIC for tasks like image classification and segmentation.With access to over 40 years of datasets, GEE enables temporal and change analyses using functions like CCDC, EWMACD, and LandTrendr. These functions facilitate tasks such as continuous change detection, trend analysis, and vegetation analysis. Specialized algorithms like VCT and VERDET are available for analyzing forest disturbances and vegetation changes over time.\n\n\n\nFigure (1): Different supporting functions within GEE. (fig.3?)\n\n\n\n\n5.2.0.3 GEE Aplication\nThis platform presents a variety of applications, some of which will be depicted in the figure below along with the frequency of studies related to them. I will provide demonstrations for some of these applications.\n\nVegetation\n\nGEE’s computational efficiency supports large-scale and long-term vegetation monitoring, exemplified by studies mapping vegetation dynamics in Queensland, Australia, and detecting degradation in Rondônia, Brazil, with reported accuracies of 82.6% and 68.1%-85.3% respectively.\n\nUrban\n\nGEE facilitates long-term monitoring of urban dynamics, including expansion mapping, climate zone monitoring, 4-D modeling, green space classification, and heat island identification, with studies showcasing its efficiency and accuracy in assessing urban growth and Surface Urban Heat Island (SUHI) effects.\n\nLand Cover\n\nGEE provides extensive remote sensing datasets for land cover mapping, dynamics monitoring, coastal mapping, and wetland classification, with studies showcasing its effectiveness in regions like northern China and Mato Grosso, Brazil, achieving over 80% accuracy using various algorithms and satellite imagery integration.\n\nNatural disasters\n\nGEE facilitates real-time and long-term analysis of remotely sensed data, enabling monitoring, forecasting, and resilience studies of natural disasters that cause destruction to both environment and human life such as droughts, floods, wildfires, landslides, hurricanes, and tsunamis, with studies showcasing its effectiveness in regions like Punjab, Pakistan, and Bangladesh.\n\n\n\nFigure (2): applications of GEE\n\n\n\n\n5.2.0.4 Advantages and limitations\n\n\n\nFigure (3): Advantages and limitation of GEE source: (Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review)"
  },
  {
    "objectID": "week 6.html#reflection",
    "href": "week 6.html#reflection",
    "title": "5  Week 5 GEE: Mapping the world",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nIt’s clear that Google Earth Engine (GEE) serves as a powerful tool for processing and analyzing large geospatial datasets, particularly in the realm of remote sensing. The platform’s cloud-based infrastructure and access to extensive satellite imagery and geospatial data make it invaluable for conducting comprehensive studies and analyses across various domains.Google Earth Engine plays a crucial part in advancing research and analysis in the field of remote sensing, offering powerful capabilities and a wide range of applications. Despite its limitations and that it’s not widely used, GEE remains a valuable platform for researchers and analysts seeking to leverage geospatial data for various studies and analyses."
  },
  {
    "objectID": "week 6.html#reference",
    "href": "week 6.html#reference",
    "title": "5  Week 5 GEE: Mapping the world",
    "section": "5.4 Reference",
    "text": "5.4 Reference\nGoogle Earth Engine for geo-big data applications: A meta-analysis and systematic review. (Tamiminia H,Salehi B 2020)\nGoogle Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and Brian Brisco. 2020. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing 164 (June): 152–70. https://doi.org/10.1016/j.isprsjprs.2020.04.001."
  },
  {
    "objectID": "week 7.html#summary",
    "href": "week 7.html#summary",
    "title": "6  Week 6 Classification I: Navigating Machine learning",
    "section": "6.1 summary",
    "text": "6.1 summary\nDuring the lecture, we were introduced to classification using remotely-sensed data, exploring various approaches to applying classification and different methods for analysis using machine learning tools like CART and Random Forest. We also reviewed several papers previously examined in the course, discussing the methodologies employed in these studies. Following that, we delved into applying these methods to our own research studies.\n\n6.1.0.1 CART\nClassification and Regression Tree is a binary tree based predictive model that performs decisions based on logical if-else scenarios. It recursively splits training data into groups based on splitting criteria (gini index or RMSE) until reaching leaf nodes (nods represent decision function) that achieve the highest possible level of accuracy.\n\n\n\nFigure (1): CART. (Malviya 2020)\n\n\n\n\n6.1.0.2 Random Forest\nAn ensemble classifier that generates multiple CART trees and by uses voting mechanism it concludes predictions. Random Forest is an extension of bagging approach whereby it uses both bagging and random subset selection of features to create uncorrelated set of trees reducing overfitting overall. The number of trees is a hyperparameter that can be tweaked in addition to other parameters like the number of random variables are used for building each tree.\n\n\n\nFigure (2): Random Forest. (Kalam 2021)\n\n\n\n\n6.1.0.3 Support Vector Machine(SVM)\nA supervised learning algorithm that is utilised for regression and classification problems. SVM finds the most ideal hyperplane during training to separate classes with as few misclassified datapoints as possible. It selects support vectors based on extreme points that aid in hyperplane creation using parameters like cost parameter C, Gamma, and kernel functions to maximise margin between classes. Hyperparameter tuning is crucial in finding the most optimal values of C and Gamma parameters which significantly influence support vector selection and SVM performance. Linear kernel is the default option for most use cases, however, SVM uses the kernel trick to apply nonlinear kernel whenever is needed.\n\n\n\nFigure (3): Support Vector Machine (SVM). (Mueller 2021)"
  },
  {
    "objectID": "week 7.html#application",
    "href": "week 7.html#application",
    "title": "6  Week 6 Classification I: Navigating Machine learning",
    "section": "6.2 Application",
    "text": "6.2 Application\nMachine learning is extensively employed across various domains including finance, trading technologies, healthcare, and traffic prediction. Exploring its application, particularly in image classification within remote sensing analysis using satellite images, is of great interest for me. Specifically focusing on urban environments, this section compares two studies: “Analysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India” (Loukika, Keesara, and Sridhar 2021)and “Mapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine”. (Pech-May et al. 2022) Both studies evaluate CART, Random Forest, and SVM algorithms for detecting land use and land cover (LULC) patterns.\nAnalysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India:\nThis study aims to utilize machine learning algorithms on Google Earth Engine to classify land use and land cover (LULC) in the Munneru River Basin, India, comparing support vector machine (SVM), random forest (RF), and classification and regression trees (CART). Leveraging Earth observation data from Landsat-8 and Sentinel-2 satellite images, the analysis meticulously considers spatial resolutions and cloud cover criteria. Spectral bands from these satellites are used to classify LULC into five primary classes: water bodies, forests, barren lands, vegetation, and built-up areas. Orthorectified images with minimal cloud cover are processed using cloud mask algorithms, and yearly means of NDVI and NDWI indices are calculated to support classification. RF, CART, and SVM are then applied for comprehensive LULC analysis of the study area.\n\n6.2.0.1 Method\nOrthorectified images with minimal cloud cover served as the primary input for classification. Cloud shadow and cover were removed using a cloud mask technique, followed by eliminating contaminated pixels. Yearly means of normalized difference vegetation index (NDVI) and normalized difference water index (NDWI) were computed. Landsat and Sentinel data were merged into composite images using the median filter. Training polygons were generated from high-resolution Google Earth images, evenly distributed across five land use classes. These polygons were loaded into Google Earth Engine (GEE) as a feature collection table. Machine learning algorithms such as Random Forest (RF), Classification and Regression Trees (CART), and Support Vector Machine (SVM) were trained using Landsat-8 and Sentinel-2 images to classify land use and land cover (LULC).\n\n\n\nFigure (4): Methodology for LULC classification on the GEE platform.\n\n\n\n\n6.2.0.2 Discussion\nSVM, CART, and RF were employed to classify LULC using Landsat-8 and Sentinel-2 images on the GEE platform. Temporal aggregation methods were utilized to address gaps in cloudy images. NDWI and NDVI were utilized as additional inputs for LULC classification, representing water bodies and vegetation characteristics. A total of 575 training sites were utilized, with each class receiving 80-95 training samples and 65-80 validation samples. The best cross-validation factor for CART was determined to be 5 or 10. RF classification showed higher accuracy with a number of trees ranging from 50 to 100, with 100 trees yielding satisfactory results in this study. Important parameters such as kernel type, gamma value, and cost were considered in SVM classification. CART had a tendency to misclassify vegetation as built-up, water bodies, or forest in 2016 and 2018, and as barren land or water bodies in 202012. SVM slightly misclassified vegetation as forest, built-up, or water bodies in 2016 and as built-up or forest in 2018. However, SVM performed well in 2020, except for some forest and built-up areas. RF outperformed the other two classifiers in all three years.\n\n\n\nFigure (5): LULC maps of Landsat-8 images using SVM, RF, and CART classifiers for the years 2016, 2018, and 2020.\n\n\nIn terms of accuracy, RF outperformed SVM and CART for both Landsat-8 and Sentinel-2 images7. The overall accuracy for Landsat-8 was 94.85% for RF, 90.88% for SVM, and 82.88% for CART. For Sentinel-2, the overall accuracy was 95.84% for RF, 93.65% for SVM, and 86.48% for CART. The kappa coefficients, which measure the agreement between predicted and observed categorizations, were also highest for RF.\n\n\n\nFigure (6): Kappa coefficient and overall accuracy of Landsat-8 and Sentinel-2 for various machine learning classifiers.\n\n\nMapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine:\nThe objective of this study is to evaluate the effectiveness of optical satellite images for land and land-cover mapping. The study area, situated in the eastern region of Tabasco, Mexico, covers towns like Balancán, Emiliano Zapata, and Tenosique. Characterized by abundant aquifers and sediment accumulation from streams, rivers, and lagoons, the area experiences a hot-humid climate. Utilizing Sentinel-2 satellite imagery via Google Earth Engine, the study established two annual time series spanning 2017 to 2019, aligning with crop cycles and regional weather patterns. The methodology involved mapping crops and land use using spectral indices and machine learning algorithms (SVM, RF, CART). Detailed tables depict land-use coverage across three zones for various seasons, showcasing changes in cropland, shrubland, water bodies, and more. Comparative analysis of classification errors for corn and sorghum crops using SVM, RF, and CART was conducted.\n\n\n6.2.0.3 Method\nIn the image selection phase, cloud masking using the QA60 band was employed to remove pixels with small accumulations of dense and cirrus clouds. A combination of reflectance thresholds and morphological operations was utilized to identify thick clouds and cirrus clouds, respectively. Then preprocessing phase involved calculating spectral indices for masked images, including NDVI, GNDVI, EVI, SAVI, and NDMI for vegetation detection, and NDWI for water bodies. Image correction techniques such as mosaicking and histogram reduction were applied to create mosaics of the study area, allowing for data aggregation over time. And the Supervised classification phase involved identifying main land types through visual analysis and applying RF, SVM, and CART algorithms to classify crops and soil types. Separate datasets were created for different crop cycles, and the dataset was divided into 70% for training and 30% for evaluation to avoid overtraining. SVM, RF, and CART algorithms were evaluated with different configurations to enhance classification efficiency.\n\n\n\nFigure (7): Proposed methodology for land-cover classification.\n\n\n\n\n6.2.0.4 Discussion\nThe SVM, RF, and CART classification algorithms were evaluated using different configurations on the Google Earth Engine (GEE) platform to enhance classification efficiency. For SVM, a kernel with a radial and gamma base function of 0.7 was employed, along with a cost of 30. Training occurred during both spring–summer and autumn–winter seasons. RF was configured to limit random forest trees to 20, minimizing misclassifications. The base GEE configuration was used for CART due to its lower classification error rate.\nTwo primary categories were defined: (1) crop types (including corn and sorghum) and (2) land use types (such as water bodies, urban areas, and tropical rainforest). The study assessed the accuracy of these classifications using overall training accuracy (OA) and the kappa index (KI). SVM performed exceptionally well, achieving an OA and KI of 0.996% in both seasons. RF also showed strong performance, with an OA and KI greater than 0.990 in spring–summer and 0.96% and 0.95% in autumn–winter. CART achieved an OA of 0.94% and a KI of 0.92% in the first season and 0.98% and 0.97% in the second season.\n\n\n\nFigure (8): Overall accuracy (OA) and Kappa index (KI) of the seasons.\n\n\nConsider the limitations of data sources. The SIAP collects crop data based on planted hectares, ignoring crops that do not sprout or grow. Consequently, the margins of error between the algorithm-detected hectares and SIAP data are substantial. While SVM outperformed actual data, there may still be errors due to occasional cultivation of small or intermittent crop lands.\n\n\n\nFigure (9): Percentage of corn and sorghum crop error by each classification method.\n\n\n\n\n6.2.0.5 Conclusion\nBoth studies outlined the classification methodology for land cover and land use (LULC), the first study methodology focused more on the preprocessing, feature selection, the preparation of training datasets, and the classifier performance evaluation. This study goes into the details of the classification process, including the selection of input data, cloud masking techniques, the methods of temporal aggregation, and spectral index calculation. Furthermore, This study conduct a comparison between the results of CART, RF, SVM and conclude that RF classifier is better than the other two. For this study the accuracy assessment is applied using OA. The second study discusses image selection, pre-processing, and supervised classification phases, then it goes into details about cloud masking and spectral index calculation, image correction, and classifier estimation. OA and KI metrics are used for the accuracy assessment. While both of studies go into details of the analysis but I found that the first one is more comprehensive about the classification process and performance evaluation."
  },
  {
    "objectID": "week 7.html#reflection",
    "href": "week 7.html#reflection",
    "title": "6  Week 6 Classification I: Navigating Machine learning",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nDuring my journey of learning about ML applications in remote sensing, at first I found the concept quite hard. However, when I started to explore the research paper topic more, things started to become clearer. I wad intrigued to see how the concepts that we have been covering these last few weeks like image correction and spectral index, will be used and combined with ML in real-life analysis. For example, l learned more about the CART method of dividing data into group, which although is simple but it has its limited by high-dimensional data. On the other hand, because of RF approach, which is based on combining multiple CART trees, RF is has the ability to improve accuracy while reducing overfitting. SVM can create an optimal hyperplane for class separation with a very minimal amount of misclassification, particularly in the dataset that has many dimensions. Finally, to choose which method is more suitable for any analysis really depends on several factors, like the nature of the dataset, desired interpretability, etc….."
  },
  {
    "objectID": "week 7.html#reference",
    "href": "week 7.html#reference",
    "title": "6  Week 6 Classification I: Navigating Machine learning",
    "section": "6.4 Reference",
    "text": "6.4 Reference\nAnalysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India (Loukika K, Keesara V 2021).\nMapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine (Pech-May F, Aquino-Santos R 2022).\n\n\n\n\nKalam, Nazhim. 2021. “Random Forest Algorithm Quick Breakdown.” https://nazhimkalam.medium.com/random-forest-algorithm-1c9235e9bc84.\n\n\nLoukika, Kotapati Narayana, Venkata Reddy Keesara, and Venkataramana Sridhar. 2021. “Analysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India.” Sustainability 13 (24): 13758. https://doi.org/10.3390/su132413758.\n\n\nMalviya, Nikita. 2020. “DECISION TREE.” https://medium.com/analytics-vidhya/decision-tree-2855f7e198f0.\n\n\nMueller, Vincent. 2021. “Support Vector Machines, Illustrated.” https://towardsdatascience.com/support-vector-machines-illustrated-b48a32c56388.\n\n\nPech-May, Fernando, Raúl Aquino-Santos, German Rios-Toledo, and Juan Pablo Francisco Posadas-Durán. 2022. “Mapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine.” Sensors 22 (13): 4729. https://doi.org/10.3390/s22134729."
  },
  {
    "objectID": "week 8.html#summary",
    "href": "week 8.html#summary",
    "title": "7  week 7 classification II: Indepth",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis lecture builds upon the foundational concepts covered in last week’s lecture, which focused on CART, Random Forest, and SVM. However, this time, we’ll cover additional principles such as Object-Based analysis, Sub-Pixel analysis, cross validation, and Spatial cross validation but because object-based analysis and sub-pixel analysis are new to me I will talk about them in my diary.\nObject-Based Image analysis\nObject-based image analysis (OBIA) involves grouping pixels into objects based on spectral similarity or external variables like ownership or soil type. These objects have various attributes such as spectral, shape, and neighborhood characteristics. OBIA allows for the inclusion of landscape knowledge by applying rules to classify objects. For example, identifying a group of trees, grass, and water near dense housing as a city park or distinguishing between forest and individual trees. Compared to traditional spectral image analysis, OBIA offers improved accuracy and detail in classification. (“Object-Based Image Analysis,” n.d.)\n\n\n\nFigure (1): Object-based Analysis. (Stefan Lang, n.d.)\n\n\nSub-Pixel analysis\nSub-pixel analysis in remote sensing involves examining the spectral composition within individual pixels to detect subtle changes that may not be apparent at the pixel level. By analysing the fractions or proportions of different land cover types within a single pixel, known as endmembers, using advanced algorithms like spectral unmixing, sub-pixel analysis offers a more detailed understanding of landscape dynamics, especially in areas where different land cover types are mixed within a pixel. This approach enhances change detection accuracy and sensitivity. Sub-pixel processing addresses the possibility of a pixel belonging to different classes in an image segmentation context, increasing the resolution of original images."
  },
  {
    "objectID": "week 8.html#application",
    "href": "week 8.html#application",
    "title": "7  week 7 classification II: Indepth",
    "section": "7.2 Application",
    "text": "7.2 Application\nFor the application part and after giving a brief about Sub-Pixel analysis and Image-based analysis I will be comparing two studies where each one of them used one of the previous method these two studies are: Sub-pixel change detection for urban land-cover analysis via multi-temporal remote sensing images (DU et al. 2014) and Object-based land cover change detection for cross-sensor images (Qin et al. 2013)\nSub-pixel change detection for urban land-cover analysis via multi-temporal remote sensing images:\n\n7.2.0.1 Method:\nThe proposed method employs an unmixing algorithm to ascertain the proportions of endmembers within a pixel, based on the V-I-S model. Unlike traditional methods yielding binary change results, sub-pixel analysis delves into the variability within pixels, considering all endmembers. Decision-level fusion techniques are used to integrate differential information and determine changes, including class transitions, direction, and intensity. The method involves four key steps: spectral unmixing, differential information generation, change determination based on defined rules, and analysis of change intensity.\n\n\n7.2.0.2 Steps:\n\nSpectral Unmixing:\n\nInitially, the BPNN unmixing algorithm is employed to produce the abundance of each endmember within a single pixel in images captured at two different dates.\n\nDifferential Information Generation:\n\nThe discrepancy in abundance for all endmembers within a pixel between two dated images is computed, comprising K fractions represented as Dk.\n\nChange Determination:\n\nDetermining change information at the sub-pixel level involves analysing the change indicator to identify changed pixels. This process entails thresholding the change magnitude image of all pixels to create a change map.\n\nChange Intensity Analysis:\n\nChange intensity, a measure of change probability, aids in detecting potential changes. After identifying changed pixels, intensity is categorized into different levels. Higher intensity values identify significant changes, indicating a greater likelihood of real changes within urban areas.\n\n\n\nFigure (2): Flowchart of the proposed sub-pixel level change detection approach.\n\n\n\n\n7.2.0.3 Experiment:\nThe study conducted experiments primarily focusing on multi-temporal China-Brazil earth resources satellite images of Shanghai city. These images, captured on March 7, 2005, and May 7, 2009, covered a 1000 × 1000 pixel area, including urban and Pudong New District. Land-cover changes observed mainly involved built-up areas and vegetation, with minor changes in soil and water. Abundance maps of different end members were analyzed for both dates. The proposed method demonstrated higher overall accuracy (89.86%) and kappa coefficient (0.7791) compared to other methods like CVA and PCA, with notable reductions in commission and omission rates. This suggests the effectiveness of the proposed approach in accurately detecting land-cover changes over time.\n\n\n7.2.0.4 Results and Findings of the Experiment:\n\nThe proposed method yields dependable results, aligning with ground truth data and photo interpretation. It offers detailed insights such as change transition, direction, and intensity to decision-makers. Changes are quantitatively evaluated, enhancing the delineation of relevant change areas. Additionally, the change intensity analysis provides rich supplementary information, particularly highlighting high probability regions, which closely correspond to actual changes.\nThe change matrix illustrates the transitions in land cover among different categories. Analysis reveals that between 2005 and 2009, land-cover alterations predominantly involve shifts from low-albedo to high-albedo (5498 pixels), soil to high-albedo (4157 pixels), low albedo to soil (4008 pixels), and soil to vegetation (2806 pixels). These findings underscore the significant influence of urbanization on land-cover changes in urban regions, particularly evident in transitions from low-albedo to high-albedo and soil to vegetation.\nThe findings obtained through sub-pixel level detection offer a higher level of completeness and accuracy compared to pixel-level techniques, offering a wealth of change information to aid decision-making and field assessments. However, methods such as CVA and PCA-based approaches exhibit some errors, notably omission errors, leading to a reduction in overall change detection accuracy.\n\n\n\n\nFigure (3): Binary change detection maps obtained by different approaches: (a) The proposed method; (b) CVA; and (c) PCA.\n\n\n\n\n\nFigure (4): Accuracy and errors of change detection approaches.\n\n\nObject-based land cover change detection for cross-sensor images:\nThe study focuses on Daqing, located in northeastern China’s Heilongjiang province, known for its diverse land cover and significant land cover changes from the 1960s to the 1990s due to petroleum exploitation. However, environmental quality has improved since the 1990s due to the local government’s eco-friendly initiatives. The study used Landsat 5 TM and IRS-P6 LISS3 images from 1990 and 2006, respectively, for land cover change detection, supplemented with QuickBird images, land use survey maps, field trips, and interviews for data collection and validation.\n\n\n7.2.0.5 Method\nThe suggested method involves preprocessing of data, segmenting the images, classifying objects, and accuracy assessment.\nImage preprocessing and transformation\nTo ensure compatibility between images, a subset of the IRS image overlapping with the TM image is chosen as T2 and georeferenced to it with an RMSE of under 0.6 pixels. T2’s pixel resolution is adjusted to match the TM images at 30 meters. Similarly, a subset of the TM images overlapping with T2 is selected as T1. Image transformation methods have been used in land cover change detection to reduce redundancy and enhance features. This study introduces a novel approach allowing for classification just once. Eigenvalue analysis suggests that the first six bands contain the most significant information, hence they are utilized for subsequent analysis.\nObject-based classification\n\nImage segmentation:\n\nImage segmentation is vital for analyzing remote sensing data. Definiens Professional offers multi-resolution segmentation, a technique merging regions from one-pixel objects upward, considering both pixel value and texture. In this study, it’s applied using the first six bands of a PCA-transformed image. Parameters like homogeneity, shape, and compactness are adjusted based on visual inspection, resulting in successful partitioning of land cover change patches into image objects.\n\nImage–object classification:\n\nObject-based image classification aims to assign components to specific categories. However, conventional classification schemes struggle to capture dynamic land cover changes. A new classification scheme is devised, describing changes between land cover types. Significant spectral and textural differences between images, especially in water areas, prompt the inclusion of subclasses based on water color. Visual interpretation of images aids in selecting 247 objects as samples for training and validation. Object-based classification often employs non-parametric algorithms due to the complexity of image objects’ attributes. In this study, NN classification is applied based on spectral attributes and textural parameters, utilizing the digital number (DN) values of six principal bands for segmentation. The shape of image segments is disregarded as it doesn’t provide significant information related to land cover change types.\n\n\n\nFigure (5): Workflow of the proposed approach.\n\n\n\n\n7.2.0.6 Results\nDue to challenges in achieving precise registration accuracy using pixel-based image analysis, especially with cross-sensor images of varying resolutions, alternative validation strategies are needed. In this study, 62 validation polygons comprising 7710 pixels are chosen through visual interpretation to assess object-based classification accuracy. The below table illustrates the confusion matrix as well as detailed accuracy information which reveals highest producer’s accuracies for water-to-water and farmland-to-builtup classes. However, errors occur in distinguishing wetland-to-water and water-to-wetland changes, likely due to spatial confusion during image segmentation. Similarly, confusion between wetland-to-builtup and farmland-to-builtup changes is observed due to spectral similarities. Despite some errors, overall accuracy and kappa coefficient are satisfactory at 83.42% and 0.82, respectively, indicating the effectiveness of the proposed approach in land cover change detection.\n\n\n\nFigure (6): Confusion matrix and accuracy of the land cover change map.\n\n\nThe figure depicts the land cover changes in the study area from 1990 to 2006, presenting both current and dynamic land cover information. Visual analysis reveals that the predominant change is ‘farmland-to-farmland’, indicating sustained farmland preservation. Additionally, there’s no notable decline in wetlands, suggesting their conservation during the period. Despite rapid economic growth, urban sprawl appears limited, with few areas transitioning from farmland to urban. Overall, the map indicates effective protection of farmland and wetlands amidst China’s urbanization, highlighting successful land management strategies in the region.\n\n\n\nFigure (7): Land cover change map with vector layers: water lines include rivers and irrigation channels; the major roads are highways which cross the study area.\n\n\n\n\n7.2.0.7 Conclusion\nBoth studies propose distinct methodologies for analyzing land cover changes using remote sensing data. Study 1 focuses on sub-pixel level change detection, employing an unmixing algorithm to ascertain endmember proportions within pixels and decision-level fusion techniques to identify changes based on defined criteria. Results from experiments on multi-temporal China-Brazil earth resources satellite images demonstrate reliable change detection, providing detailed change information and achieving higher accuracy compared to traditional methods. On the other hand, Study 2 introduces a novel object-based classification approach involving preprocessing, image segmentation, and non-parametric classification algorithms. Despite challenges in distinguishing certain change types, the method demonstrates satisfactory accuracy and offers insights into land cover changes over time. While Study 1 offers more detailed change information, Study 2’s approach may be more suitable for analyzing complex land cover changes. Ultimately, the choice between the two methods depends on the specific requirements and objectives of the analysis."
  },
  {
    "objectID": "week 8.html#reflection",
    "href": "week 8.html#reflection",
    "title": "7  week 7 classification II: Indepth",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nToday’s lecture was interesting for me because it has expanded my understanding of remote sensing techniques that goes beyond CART, RF, and SVM. Honestly, I got lost in which of these two approaches is more fascinating.  In OBIA we approach image analysis is not only based on individual pixels but also by making them into groups of meaningful objects. While Sub-Pixels analysis mainly work on allowing the detection of changes on individual pixels level. The Sub-Pixels method can be useful to use in case of urban environments or transitional ecosystems. I think that in the future this could help in various fields such as environmental monitoring, urban planning (which I find interesting), and disaster management. Furthermore, I believe that it is important to understand the use of these techniques as they might have a crucial role in addressing new challenges such as climate change impacts, habitat loss and urbanization by helping policymakers and stakeholders with detailed information that would help in the decision-making process."
  },
  {
    "objectID": "week 8.html#reference",
    "href": "week 8.html#reference",
    "title": "7  week 7 classification II: Indepth",
    "section": "7.4 Reference",
    "text": "7.4 Reference\nSub-pixel change detection for urban land-cover analysis via multi-temporal remote sensing images.(DU P, LIU S 2014)\nObject-based land cover change detection for cross-sensor images. ( Y. Qin, Z. Niu 2013).\n\n\n\n\nDU, Peijun, Sicong LIU, Pei LIU, Kun TAN, and Liang CHENG. 2014. “Sub-Pixel Change Detection for Urban Land-Cover Analysis via Multi-Temporal Remote Sensing Images.” Geo-Spatial Information Science 17 (1): 26–38. https://doi.org/10.1080/10095020.2014.889268.\n\n\n“Object-Based Image Analysis.” n.d. https://www.gim-international.com/content/article/object-based-image-analysis.\n\n\nQin, Y., Z. Niu, F. Chen, B. Li, and Y. Ban. 2013. “Object-Based Land Cover Change Detection for Cross-Sensor Images.” International Journal of Remote Sensing 34 (19): 6723–37. https://doi.org/10.1080/01431161.2013.805282.\n\n\nStefan Lang, University of Salzburg. n.d. “Object-Based Image Analysis - An Introduction.” https://eo4geocourses.github.io/PLUS_OBIA-Introduction/#/1."
  },
  {
    "objectID": "week 9.html#summary",
    "href": "week 9.html#summary",
    "title": "8  week 8 SAR: Reveling SAR",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nIn today’s lecture, we explored Synthetic Aperture Radar (SAR) technology and compared it with the optical imagery we’ve been using throughout this module. We deep dived into the details of SAR, including its polarization and wavelength, and discussed how SAR functions. Specifically, we focused on change detection in SAR images. While we discussed one method in depth during the lecture, the slides presented various other methods for change detection.\nSynthetic Aperture Radar (SAR)\n(SAR) is a radar technology utilized for generating two-dimensional images or three-dimensional reconstructions of various objects, including landscapes. SAR achieves finer spatial resolution by employing the motion of the radar antenna over a target area, which differs from the stationary beam-scanning radars traditionally used. Functioning as an active sensor, SAR transmits microwave signals and captures the signals reflected, or backscattered, from the Earth’s surface. This capability enables SAR to produce high-resolution images using relatively compact antennas. Also SAR possesses unique attributes such as cloud-penetrating capabilities.\n\n\n\nFigure (1): Synthetic Aperture Radar (SAR). (“SAR,” n.d.)\n\n\nSAR Polarization and Scattering Mechanisms\nPolarization refers to the orientation of the plane in which the transmitted electromagnetic wave swings. SAR sensors usually transmit linearly polarized whereby the advantage of radar sensors is that signal polarization can be precisely controlled on both transmit and receive. Signal strength of different polarizations carries information about the imaged surface structure, based on different types of scattering which are rough surface, volume, and double bounce scattering. (Earth Science Data Systems 2020)\n\nRough surface scattering, such as that caused by bare soil or water, is most sensitive to VV scattering.\nVolume scattering, for example, caused by the leaves and branches in a forest canopy, is most sensitive to cross-polarized data like VH or HV.\nThe last type of scattering, double bounce, is caused by buildings, tree trunks, or inundated vegetation and is most sensitive to an HH polarized signal.\n\nLastly, as wavelength changes signal penetration depth, it drives the amount of signal attributed to different scattering types.\n\n\n\nFigure (2): SAR Polarization and Scattering. (Earth Science Data Systems 2020)\n\n\nChange Detection in SAR\nChange detection in (SAR) images involves identifying changes in land cover over time by examining and contrasting two or more SAR images captured at different times but covering the same geographic region. This method is highly beneficial in remote sensing applications due to SAR’s ability to acquire images regardless of weather conditions and its capability to penetrate clouds and darkness. This technique is highly valuable to a large number of applications, such as flood detection, disaster monitoring, urban planning, and land cover data monitoring. However, the inherent speckle noise in SAR images can lead to false alarms and misdetections.(Shukla et al. 2023)"
  },
  {
    "objectID": "week 9.html#application",
    "href": "week 9.html#application",
    "title": "8  week 8 SAR: Reveling SAR",
    "section": "8.2 Application",
    "text": "8.2 Application\nIn this application section, I’ll deep dive into the study of change detection in SAR images. While we’ve explored various techniques and methods for change detection in previous lectures, I’ve chosen to focus on a methodology that hasn’t been extensively covered. My aim is to introduce a new approach that offers insights beyond what’s been discussed in class, enriching my understanding and exploration of SAR image analysis for change detection purposes in my last learning diary entry.\nChange Detection in Synthetic Aperture Radar Images based on Image Fusion and Fuzzy Clustering: (Gong, Zhou, and Ma 2012)\nThis paper highlights the challenges in change detection in Synthetic Aperture Radar (SAR) images, primarily due to speckle noise. The paper suggests the use of the ratio operator over subtraction for SAR images to handle speckle noise and calibration errors. To enhance change detection, it introduces image fusion to combine mean-ratio and log-ratio images. Furthermore, it proposes a robust fuzzy clustering algorithm that incorporates spatial context to improve resistance against noise and enhance change detection accuracy.\nMethodology\nThe paper methodology proposed a new approach for change detection in SAR images, made of two main steps: generating a difference image through image fusion and detecting changed areas in the fused image using an improved Fuzzy C-Means (FCM) algorithm:\n\nImage Fusion techniques are used to improve image quality, mainly focusing on pixel-level fusion using methods like the discrete wavelet transform (DWT). The DWT separates frequencies in time and space in efficient manner, making it suitable for change detection tasks with large volumes of image data. The fusion process consists of DWT computation of each source image, fusing corresponding coefficients, and applying inverse DWT to obtain the fused result.\nFuzzy Clustering: Clustering to differentiate between changed and unchanged areas in the fused image is performed by using an improved FCM algorithm. Traditional FCM algorithms often lack robustness to noise, prompting the development of variants like FCM_S and FGFCM incorporating local spatial information. However, parameter tuning is needed for these approaches and may still be sensitive to noise. To address this, a robust Fuzzy Local Information C-Means (FLICM) algorithm is proposed. It utilises a fuzzy factor to balance noise robustness and maintain image detail. Additional improvement is made by replacing spatial distance with the local coefficient of variation to better handle noise resulting in the reformulated RFLICM algorithm.\n\n\n\n\nFigure (3): Flowchart of the proposed change detection approach.\n\n\n\n8.2.0.1 Experiments\nTo assess the proposed method effectiveness, the paper ran experiments on three different data sets. I will discuss in detail one of them as I believe it is adequate for this section:\n\nBern data set\n\nWavelet image fusion effectiveness in generating difference images was evaluated against mean-ratio and log-ratio methods. The fused difference image captured step changes, suppressing unchanged regions using wavelet coefficients from the log-ratio image. Results showed that log-ratio operator PCC was 99.27% for Otsu and 99.24% for K-means, while the proposed approach achieved the highest PCC (99.35% for Otsu and 99.36% for K-means) and kappa (0.781 for Otsu and 0.784 for K-means). While mean-ratio image produced more spots in change detection due to speckle effects, the log-ratio image had fewer spots but suffered from information loss in changed areas. The wavelet fusion image reduced errors in change detection results. In the second experiment, the RFLICM algorithm impact on SAR-image change detection using wavelet fusion difference images was evaluated. Traditional FCM produced many spots due to its lack of spatial context consideration. However, FLICM and RFLICM yielded robust change detection maps, with RFLICM outperforming FLICM and FCM with a PCC of 99.68%, 99.66%, and 99.37% respectively.\n\n\n\nFigure (4): Change detection results of the Bern data set based on the three difference images obtained by Otsu. (a) Based on the mean-ratio operator. (b) Based on the log-ratio operator. (c) Based on wavelet fusion.\n\n\n\n\n\nFigure (5): Change detection results of the Bern data set based on the three difference images obtained by K-means. (a) Based on the mean-ratio operator. (b) Based on the log-ratio operator. (c) Based on wavelet fusion.\n\n\n\n\n\nFigure (6): Change detection results of the Bern data set achieved by (a) FCM, (b) FLICM, and (c) proposed RFLICM.\n\n\n\n\n8.2.0.2 Conclusion\nThe method used in change detection in (SAR) images combines wavelet image fusion and an improved fuzzy clustering algorithm, Robust Fuzzy Local Information C-Means (RFLICM). This method outperforms other methods like image differencing technique which struggles due to the speckle noise presence as well as the statistical properties of SAR images being non-robust to calibration errors. The wavelet fusion approach in the proposed method improves changed region information and suppresses background noise by integrating mean-ratio and log-ratio images. Meanwhile, the RFLICM algorithm leverages local spatial and gray information which make it more accurate and less sensitive to the probability statistics model than traditional thresholding techniques. Results show that this method outperforms traditional ones, by wavelet fusion strategy integrating with log-ratio and mean-ratio operators, and RFLICM showing fewer errors and spots in change detection results. I really like this approach because and according to my understanding this can be useful in providing a more accurate reflection for real world change detection and better suppression of background noise."
  },
  {
    "objectID": "week 9.html#reflection",
    "href": "week 9.html#reflection",
    "title": "8  week 8 SAR: Reveling SAR",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nIt was really interesting to cover and go deep in the change detection of SAR images methods. For me I feel like because I’m interested in using sattelite images and remote sensing to map the impact of conflict and to detect changes in those areas. In the lectures we talked about images enhancements methods and image fusion was one of them. I got intrigued in finding out more about image fusion methods especially the ones that we haven’t covered. For this reason, I searched for discrete wavelet transform, which was surprisingly an eye opening because I learned so much about the SAR images change detection and what are the hardships in its applications. I think this change detaction method is crucial for humanitarian organization work in detecting in a precise manner where to focus their efforts and the most damaged areas and buildings. This will eventually lead to deliver the most efficent excution plan for any humanitarian intervention. Some of the stuff that are covered in this lecture was mentioned before but briefly like polarization and scattering although they are simple but they have a great importance."
  },
  {
    "objectID": "week 9.html#reference",
    "href": "week 9.html#reference",
    "title": "8  week 8 SAR: Reveling SAR",
    "section": "8.4 Reference",
    "text": "8.4 Reference\nChange Detection in Synthetic Aperture Radar Images based on Image Fusion and Fuzzy Clustering (Gong.M, Zhou.Z 2012)\nChange Detection on SAR Images (R.A. Alagu Raja,K. Vaiyammal 2017).\nDeep Learning-Based Suppression of Speckle-Noise in Synthetic Aperture Radar (SAR) Images: A Comprehensive Review (Kant Shukla.S, K. Dwivedi.S 2023).\nDeep Despeckling of SAR Images to Improve Change Detection Performance (Ihmeida.M, Shahzad.M 2023).\n\n\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nGong, Maoguo, Zhiqiang Zhou, and Jingjing Ma. 2012. “Change Detection in Synthetic Aperture Radar Images Based on Image Fusion and Fuzzy Clustering.” IEEE Transactions on Image Processing 21 (4): 2141–51. https://doi.org/10.1109/TIP.2011.2170702.\n\n\n“SAR.” n.d. http://labs.cas.usf.edu/geodesy/sar.html.\n\n\nShukla, Ashwani Kant, Sanjay K. Dwivedi, Ganesh Chandra, and Raj Shree. 2023. “Deep Learning-Based Suppression of Speckle-Noise in Synthetic Aperture Radar (SAR) Images: A Comprehensive Review.” In, edited by Amit Kumar, Gheorghita Ghinea, Suresh Merugu, and Takako Hashimoto, 693–705. Singapore: Springer Nature. https://doi.org/10.1007/978-981-19-2358-6_62."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Achour, Hammadi, Ahmed Toujani, Hichem Trabelsi, and Wahbi Jaouadi.\n2022. “Evaluation and Comparison of Sentinel-2 MSI, Landsat 8 OLI,\nand EFFIS Data for Forest Fires Mapping. Illustrations from the Summer\n2017 Fires in Tunisia.” Geocarto International 37 (24):\n7021–40. https://doi.org/10.1080/10106049.2021.1980118.\n\n\nAimaiti, Yusupujiang, Christina Sanon, Magaly Koch, Laurie G. Baise, and\nBabak Moaveni. 2022. “War Related Building Damage Assessment in\nKyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical\nImages.” Remote Sensing 14 (24): 6239. https://doi.org/10.3390/rs14246239.\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing Platform for\nRemote Sensing Big Data Applications: A Comprehensive Review.”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\n“Atmospheric Remote Sensing Toolkit.” n.d. https://sees-rsrc.science.uq.edu.au/rstoolkit/en/html/atmospheric/resources/fundamentals/corrections/physics-based.html.\n\n\nDU, Peijun, Sicong LIU, Pei LIU, Kun TAN, and Liang CHENG. 2014.\n“Sub-Pixel Change Detection for Urban Land-Cover Analysis via\nMulti-Temporal Remote Sensing Images.” Geo-Spatial\nInformation Science 17 (1): 26–38. https://doi.org/10.1080/10095020.2014.889268.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? |\nEarthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n———. 2020. “What Is Synthetic Aperture Radar? | Earthdata.”\nhttps://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nFagan, Matthew E., and Ruth S. DeFries. 2024. “Remote Sensing and\nImage Processing.” In, edited by Samuel M. Scheiner, 432–45.\nOxford: Academic Press. https://doi.org/10.1016/B978-0-12-822562-2.00060-8.\n\n\n“Geometric Corrections - AWF-Wiki.” n.d. http://wiki.awf.forst.uni-goettingen.de/wiki/index.php/Geometric_corrections.\n\n\nGISGeography. 2015. “What Is Atmospheric Correction in Remote\nSensing?” https://gisgeography.com/atmospheric-correction/.\n\n\nGong, Maoguo, Zhiqiang Zhou, and Jingjing Ma. 2012. “Change\nDetection in Synthetic Aperture Radar Images Based on Image Fusion and\nFuzzy Clustering.” IEEE Transactions on Image Processing\n21 (4): 2141–51. https://doi.org/10.1109/TIP.2011.2170702.\n\n\nIalongo, Iolanda, Rostyslav Bun, Janne Hakkarainen, Henrik Virta, and\nTomohiro Oda. 2023. “Satellites Capture Socioeconomic Disruptions\nDuring the 2022 Full-Scale War in Ukraine.” Scientific\nReports 13 (1): 14954. https://doi.org/10.1038/s41598-023-42118-w.\n\n\nKalam, Nazhim. 2021. “Random Forest Algorithm Quick\nBreakdown.” https://nazhimkalam.medium.com/random-forest-algorithm-1c9235e9bc84.\n\n\n“Landsat 8 | Landsat Science.” 2021. https://landsat.gsfc.nasa.gov/satellites/landsat-8/.\n\n\nLoukika, Kotapati Narayana, Venkata Reddy Keesara, and Venkataramana\nSridhar. 2021. “Analysis of Land Use and Land Cover Using Machine\nLearning Algorithms on Google Earth Engine for Munneru River Basin,\nIndia.” Sustainability 13 (24): 13758. https://doi.org/10.3390/su132413758.\n\n\nMalviya, Nikita. 2020. “DECISION TREE.” https://medium.com/analytics-vidhya/decision-tree-2855f7e198f0.\n\n\n“Marine Remote Sensing Toolkit.” n.d. https://sees-rsrc.science.uq.edu.au/rstoolkit/en/html/marine/resources/fundamentals/corrections/atmospheric_relative-1.html.\n\n\nMueller, Vincent. 2021. “Support Vector Machines,\nIllustrated.” https://towardsdatascience.com/support-vector-machines-illustrated-b48a32c56388.\n\n\nNiraj, K. C., Sharad Kumar Gupta, and Dericks Praise Shukla. 2022.\n“A Comparison of Image-Based and Physics-Based Atmospheric\nCorrection Methods for Extracting Snow and Vegetation Cover in Nepal\nHimalayas Using Landsat 8 OLI Images.” Journal of the Indian\nSociety of Remote Sensing 50 (12): 2503–21. https://doi.org/10.1007/s12524-022-01616-6.\n\n\n“Object-Based Image Analysis.” n.d. https://www.gim-international.com/content/article/object-based-image-analysis.\n\n\nPech-May, Fernando, Raúl Aquino-Santos, German Rios-Toledo, and Juan\nPablo Francisco Posadas-Durán. 2022. “Mapping of Land Cover with\nOptical Images, Supervised Algorithms, and Google Earth Engine.”\nSensors 22 (13): 4729. https://doi.org/10.3390/s22134729.\n\n\nQin, Y., Z. Niu, F. Chen, B. Li, and Y. Ban. 2013. “Object-Based\nLand Cover Change Detection for Cross-Sensor Images.”\nInternational Journal of Remote Sensing 34 (19): 6723–37. https://doi.org/10.1080/01431161.2013.805282.\n\n\n“Remote Sensing from Space  Paititi Research.”\nn.d. https://www.paititi.info/research-technology/remote-sensing-from-space/.\n\n\n“SAR.” n.d. http://labs.cas.usf.edu/geodesy/sar.html.\n\n\nSavenets, Mykhailo, Volodymyr Osadchyi, Kateryna Komisar, Natalia\nZhemera, and Andrii Oreshchenko. 2023. “Remotely Visible Impacts\non Air Quality After a Year-Round Full-Scale Russian Invasion of\nUkraine.” Atmospheric Pollution Research 14 (11):\n101912. https://doi.org/10.1016/j.apr.2023.101912.\n\n\n“Sentinel-2 - Missions - Sentinel Online.” n.d. https://copernicus.eu/missions/sentinel-2.\n\n\nShi, Liangliang, Zhihua Mao, Peng Chen, Sha’ou Han, Fang Gong, and\nQiankun Zhu. 2016. “Remote Sensing of the Ocean, Sea Ice, Coastal\nWaters, and Large Water Regions 2016.” In, 9999:259–63. SPIE. https://doi.org/10.1117/12.2241368.\n\n\nShukla, Ashwani Kant, Sanjay K. Dwivedi, Ganesh Chandra, and Raj Shree.\n2023. “Deep Learning-Based Suppression of Speckle-Noise in\nSynthetic Aperture Radar (SAR) Images: A Comprehensive Review.”\nIn, edited by Amit Kumar, Gheorghita Ghinea, Suresh Merugu, and Takako\nHashimoto, 693–705. Singapore: Springer Nature. https://doi.org/10.1007/978-981-19-2358-6_62.\n\n\nStefan Lang, University of Salzburg. n.d. “Object-Based Image\nAnalysis - An Introduction.” https://eo4geocourses.github.io/PLUS_OBIA-Introduction/#/1.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush,\nSarina Adeli, and Brian Brisco. 2020. “Google Earth Engine for\nGeo-Big Data Applications: A Meta-Analysis and Systematic\nReview.” ISPRS Journal of Photogrammetry and Remote\nSensing 164 (June): 152–70. https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\n“Technology  Balamis.” n.d. https://www.balamis.com/technology/.\n\n\n“Universal Sustainable Development Goals .:. Sustainable\nDevelopment Knowledge Platform.” n.d. https://sustainabledevelopment.un.org/index.php?page=view&type=400&nr=1684&menu=1515."
  }
]